Covariance matrix estimation is an important area of research in both finance and statistics. Suppose we have independent observations ${X}_{t}=(X_{1t},\dots,X_{Nt})^T$, $t=1,\dots,T$ of a $N-$dimensional random vector \(\mathbf{X}\) that has mean \(\mu\) and variance ${\Sigma}_{X} =E\pqty{(\mathbf{X} - \mu) \pqty{\mathbf{X} - \mu}'}$. The most straightforward estimator is the sample covariance estimator, which is defined as follows:
\begin{equation}
{\hat{\Sigma}_{X}}=\frac{1}{T}({X}-\bar{{X}})({X}-\bar{{X}})^{\intercal}=[\hat{\sigma}_{ij}]_{N\times N},
\end{equation}
where ${X}$ is the $N\times T$ matrix of observations; $\bar{{X}}=\frac{1}{T}{X}{1}_{T}$ is the sample time series average, with ${1}_{T}$ being a $T\times 1$ vectors of 1. However, in the high-dimensional case, where the dimension \(N\) is not negligible comparing to sample size \(T\), the sample covariance matrix is ill-conditioned and inconsistent. 

One of the structures often imposed in the high-dimensional settings is sparsity, which assumes that ${\Sigma}_{X}$ is sparse (i.e., has lots of zeros or small elements) or conditionally sparse (i.e., has lots of zeros or small elements once we condition on some variables like the common risk factors). 

Given the sparsity structure, several estimation strategies have been proposed in the literature such as banding, tapering, shrinakge, threhsolding, etc. 

Banding and tapering are applicable when $X_{it}$ are indexed and we have a way to define a ``distance'' \(d(i,j)\) between \(X_{i}\) and \(X_{j}\). For example, in time series, a natural distance is $|i-j|$ and we have reason to believe that large distance between \(X_{i}\) and \(X_{j}\) will imply lower correlation. Such structure is appropriate for applications where there are natural orderings of variables, such as time series, climatology and spectroscopy. Banding methods(\cite{bickel2008RegularizedEstimation}) keeps only elements within a \(k\)-neighourhood of each indiviual \(X_{i}\), that is 
\begin{equation*}
    \hat{\Sigma}_{B,k} = [\hat{\sigma}_{ij} \mathbf{1}_{d(i,j) \leq k}]
\end{equation*}
whereas the tapering estimator does a elementwise multiplication of \(\hat{\Sigma}_{X}\) with a positive definite matrix \(\mathbf{T}\) that has smaller element \(\mathbf{T}_{ij}\) decreases with \(d(i,j)\). 

When we don't have the information about the distance among \(i,j\),one of the viable approachs is thresholding. Let \(s_{\lambda}(\cdot)\) be a generalized thresholding operator satisfying 
\begin{enumerate*}
    \item \(\abs{s_{\lambda}(z)} \leq c \abs{y}\) for all \(\abs{z - y}\leq \lambda\)
    \item \(s_{\lambda}(z) = 0\) for \(\abs{z}\leq \lambda\)
    \item \(\abs{s_{\lambda}(z) - z} \leq \lambda\).
\end{enumerate*}
which incorporates commonly used thresholding operators such as hard thresholding, soft thresholding, SCAD, etc. Then a thresholding estimator is 
\begin{equation*}
    \hat{\Sigma}_{\lambda} = \bqty{\tilde{\sigma}_{ij}} \qq{and} \tilde{\sigma}_{ij}= 
    \begin{cases}
        \hat{\sigma}_{ii}   & i=j \\ 
        s_{\lambda}(\hat{\sigma}_{ij})  & i\neq j 
    \end{cases}
\end{equation*}

\cite{bickel2008CovarianceRegularization} develop the theory for universal thresholding which assumes that the diagonal of \(\Sigma\) is uniformly bounded. \cite{cai2011AdaptiveThresholding} proposes an adaptive thresholding estimator. They relax the uniform boundedness assumption and accounts for the varaince of estimator of each \(\hat{\sigma}_{ij}\) the establishs entry-adaptive threshold to ${\hat{\Sigma}_{X}}$. \cite{fan2013large} argue that common factors should be extract first before applying thresholding selection when there are "extremely spiked" eigenvalues in ${\hat{\Sigma}_{X}}$ (i.e., the covariance matrix is conditionally sparse). \cite{shu2019EstimationLarge} obtains the convergence rate allowing for temporal dependence. \cite{bickel2008CovarianceRegularization} also compares the convergence rates of banding estimator and thresholding estimator and by utilizing the location information, banding estimator shows a superior convergence rate. See \cite{fan2015OverviewEstimation} for a review on estimation of large dimensional covariance matrices. 

Apart from elementwise regularization methods, \cite{ledoit2004WellconditionedEstimator} and \cite{ledoit2012NonlinearShrinkage} have proposed linear and nonlinear shrainkge estimators that apply shrinakge to the eigenvalues of the sample covaraince.  The linear shrinkage does that by finding the linear combination of sample covariance and a well-conditioned matrix such as the identity matrix and nonlinear shrinkage estimator corrects the eigenvalues using the asymptotic Marcenko–Pastur distribution. 
Shrinakge estimators have been succesffuly applied the estimators in portfolio construction(\cite{ledoit2004HoneyShrunk}, \cite{ledoit2017NonlinearShrinkage}).

% \cite{bickel2008RegularizedEstimation} regularize the ${\hat{\Sigma}_{X}}$ in a way that, for a typical entity $i$, only the set of neighours defined by $\{j\in {N_i}: |i-j|\leq \rho, j=1,\dots,N \}$ have non-zero covariances with $i$. Other $j$s with $|i-j|>k$ have $\sigma_{ij}=0$. When $X_{it}$ is not indexed, we may apply some shrinkage techniques to the off-diagonal elements of ${\hat{\Sigma}_{X}}$, and achieve sparsity by setting relatively unimportant entries that are smaller than a data-driven threshold to zeros.  In general, we have ${\tilde{\Sigma}_{X}}=[\tilde{\sigma}_{ij}]_{N\times N}$, where
% where $s_{\lambda}(\cdot)$ is a shrinkage function with $\lambda$ being a tuning parameter, satisfying (1) $|s_{\lambda}(\mu)|\leq|\mu|$ for $\mu \in \mathcal{R}$; (2) $s_{\lambda}(\mu)$ if $|u|\leq \lambda$; (3) $|s_{\lambda}(\mu)-\mu|\leq \lambda$.  

% Another way of shrinkage is via penalization. \cite{yuan2007model}, \cite{d2008first}, \cite{rothman2008sparse} and \cite{friedman2008sparse} among others employ LASSO type penalty to shrink sample covariance matrix. Another seminal penalty choice is SCAD, which is proposed by \cite{fan2001variable}, can be found in \cite{fan2009network} and \cite{lam2009sparsistency}.

Although in general we won't have an ordering or know distance between \((i,j)\), we do have some idea about who might be connected with whom using augment information apart from the observations of ${X}$. To proxy for pairwise connectivity among entities, apart from purely statistical correlation, several methods have been proposed. 
\cite{hoberg2016text} use textual analysis to identify peers. \cite{kaustia2013common} identify peers analyst co-coverage, and \cite{ge2021news} identify peers using business news co-mentioning. 
Those network information gathered from other sources can help us to identify the locations of non-zeros apart from the information from ${X}$ itself. Similar location-based thresholding ideas have applied in \cite{fan2016incorporating} and \cite{brownlees2020community}. \cite{fan2016incorporating} apply a hard thresholding method in a way that $\sigma_{ij}=0$ when $i$ and $j$ are from different sector/industry. 
\cite{brownlees2020community} first detect community structure using a spectral clustering-based procedure, and then apply a block-by-block thresholding to the off-diagonal elements of ${\hat{\Sigma}_{X}}$. In particular, they do not apply and thresholding to $\hat{\sigma}_{ij}$ if $i$ and $j$ are from the same community.

We argue in this paper that we can incorporate such auxiliary information in the estimation of the covariance matrix \(\Sigma\) when they help reveal the locations of the larger elements(or nonzero elements in the strictly sparse case). The augmented thresholding estimator will relax the condition put on the sparsity of the covariance \(\Sigma\) and show superior performance. 

% These estimators use only the observations ${X}$ and ignore information about pairwise relationships beyond the observed data ${X}$, which may suffer a loss especially in a information-abundant era. When $X_{it}$ is not indexed, we are not be able to have strong implications in terms of the locations of non-zeros as in \cite{bickel2008regularized}.

In this paper, we utilize granular network information gathered from other sources that could imply the locations of non-zeros in the de-factored residuals covariance matrix. The first candidate is the new-implied network. It has been documented that common news coverage reveals information about linkages among companies, which are related to many economically important relationships like business alliances, partnerships, banking and financing, customer-supplier, and production similarity (\cite{scherbina2015economic}, \cite{schwenkler2019network}). \cite{ge2021news} document that stocks linked by news co-mentioning exhibit additional co-movement beyond what can be explained by common risk factors. Same as \cite{ge2021news}, we use news data from RavenPack Equity files Dow Jones Edition for the period between the beginning of 2004 to the end of 2015. This comprehensive news dataset combines relevant content from multiple sources, including Dow Jones Newswires, Wall Street Journal, and Barron’s  MarketWatch, which produce the most actively monitored streams of news articles in the financial system. We identify linkages among firms by news co-mentioning. The second candidate network is IBES analyst co-coverage network. \cite{israelsen2016does} documents that stocks linked by analysts exhibit excess comovement. To construct the analyst co-coverage-based adjacency matrix, we use the Institutional Brokers Estimate System (IBES) detail history files. (after getting the network, our procedure)?


Although here we are applying augment network information to the estimation of large static covariance matrix, similar idea can be extended to the estimation of large dynamic covariance matrix. For example, dynamic network information could be well incorporated into the conditioning information set in \cite{chen2019new}.

