Our goal is to estimate \(\Sigma =\variance(y)\) where \(y\) is a \(p \times 1\) random vector, say asset returns. We collect the \(T\) observations into a matrix \(Y:p\times T\). Sample covariance estimate \(\hat{\Sigma} = \frac{1}{T} (Y - \bar{Y}\mathbf{1})(Y - \bar{Y}\mathbf{1})'\) is problematic when \(p\) is not small relative to \(T\). Popular estimation strategies include factor model, shrinkage, thresholding, banding, tapering, etc.

If in addition to the observation of \(Y\), we observe a network \(G\) among the firms, where \(G_{ij}\) either takes value \(0,1\) or a score in \([0,1]\), with higher \(G_{ij}\) implying that it's more ``likely'' that the returns of firm \(i,j\) are correlated. We show that this auxiliary network can be used to improve the estimation the covariance matrix \(\Sigma\). Examples of such network include \cite{hoberg2016TextBasedNetwork}, who identifies a product similarity network from financial reports that has been shown to be more accurate than industry block diagonal matrix. As linked firms are potentially subject to similar demand shock, we have reason to believe that \(G\) contains valuable information about the comovement among the returns.
\cite{israelsen2016does} and \cite{kaustia2020CommonAnalysts} both find that companies covered by the same analysts show similarities in many unobserved dimensions, and this analyst-based network could explain excess co-movement on top of common factors. With the development of machine learning techniques such as textual analysis, we are better at acquiring information from big data. Granular linkage information among firms that used to be notoriously hard to get due to its proprietary properties,  now are becoming available to researchers. The question is, how to use those auxiliary network information to  better estimate the co-movement between assets?

% For example, such \(G\) information could come from textual analysis, that has become more and more popular in finance, for example (\cite{fan2021HowMuch}). 

This paper aims to provide ways to extract the information contained in the auxiliary \(G\) matrix to help estimate the covariance \(\Sigma\). We consider an \textit{Adaptive Correlation Thresholding} method, where we apply thresholding to the correlation matrix, with the threshold level depending on network information. More specifically, suppose we observe \(Y_{t}\) for \(t = 1 ,\dots, T\), the procedure is 
% \begin{enumerate}
    % \item 
    % \item Guided Linear Shrinkage: we adopt the linear shrinkage method, where the shrainkge targets is chosen based on the network information. This is different from previous practice of shrinking to the identity or equicorrelation matrix. 
% \end{enumerate}

\begin{enumerate}
    \item Estimate the sample covariance estimate \(\hat{\Sigma}\), and the sample correlation matrix \(\hat{R}\). 
    \item Apply the generalized thresholding function \(h(r_{ij},\tau_{ij})\) to the off-diagonal elements of \(\hat{R} = (\hat{r}_{ij})\), as in  \cite{rothman2009GeneralizedThresholding}. The novelty is now we allow the threshold \(\tau_{ij}\) to vary across elements and to depend on the network information.  Specifications we have considered for the threshold \(\tau\) are 
    \begin{itemize}
        \item Simple linear model 
            \begin{equation*}
                \tau(G_{ij}) = a + bG_{ij}
            \end{equation*}
        \item 
        The probit model
            \begin{equation*}
            \tau_{ij} = \tau\pqty{G_{ij}} = \Phi\pqty{a + b \abs{G_{ij}}}
            \end{equation*}
    \end{itemize}
    \item Estimate the unknown parameters in the \(\tau\) function by cross validation, as in \cite{bickel2008CovarianceRegularization}, \cite{cai2011AdaptiveThresholding}, where we randomly split the sample \(V\) times, for each \(v\), compute the new estimator \(\hat{\Sigma}^{1,v}_{G}\) with the first subsample, and sample covariance \(\hat{\Sigma}^{2,v}\) and the criterion is 
    \begin{equation*}
        L(a, b) = \frac{1}{V} \sum_{v}^{V} \norm{\hat{\Sigma}^{1,v}_{G} - \hat{\Sigma}^{2,v}}^{2}_{F}
    \end{equation*}
    we find \(a,b\) that minimise this criterion. 
    
    \item Then with the estimates of \(a,b\), we can estimate \(\Sigma\) on the test sample. 
\end{enumerate}

%For the second method we consider, we construct a linear shrinkage target based on the hard-thresholded version of \(\hat{\Sigma}\), call it \(\hat{\Sigma}_{H} = [\hat{\Sigma}_{ij} \mathbf{1}\Bqty{G_{ij} > 0}]\). And apply linear shrinkage with the target. 

There are several advantages of using network guided method:
\begin{enumerate}
    \item The main advantage is that we are combining economically meaningful network with market-based performance data. Comparing to purely data-driven thresholding or shrinkage methods, the method utilizes valuable information embedded in external network data, which provides more robustness and efficiency. aif our auxiliary network contains the ``real'' links from the network. The relationship identified will be more stable over time than the relationship identified from return data alone. 
    \item This method is very flexible and extensible. Although in our current analysis we only use one of the existing networks as our proxy for $G$, you are free to include many candidate networks in the \(\tau\). You may want want to include characteristics-based distances, as it has been documented that companies with similar characteristics exhibit additional co-movement on top of common risk factors (see \cite{fernandez2011spatial} for example). It also provides a way to discern which set of information is relevant based an estimate of the coefficients \(a,b\) in the thresholding level. 
    \item The networks may provide industry-level comovement that is potentially related to the ``weak factors'' components, which we intend to investigate. 
\end{enumerate}

% There are some challanges and questions as well:
% \begin{enumerate}
    % \item Since we don't observe the true \(\Sigma\), we wonder if there are some good criteria for evaluating the performance of our \(\hat{\Sigma}\). So far I am following \cite{ledoit2004HoneyShrunk} and \cite{ledoit2017NonlinearShrinkage}. But unlike the linear and nonlinear shrinkage method, our estimator is not designed for optimal performance under the Frobenius norm. 
    % \item We don't have a quality measure of the network matrix or a model for how the \(G_{ij}\) are generated. 
    % \item In simulation studies, the optimization takes some time, but the result is not yet very robust(some times it outperforms all competitors, but sometimes the optimization will be very   off-target).
% \end{enumerate}
\section{Introduction}

Covariance matrix estimation is an important area of research in both finance and economics. Suppose we have observations $\mathbf{X}_{t}=(X_{1t},\dots,X_{Nt})^T$, $t=1,\dots,T$, that are from an $N-$dimensional random vectors with $Cov(\mathbf{X})=E(\mathbf{X}\mathbf{X}^T)={\Sigma}_{X}$. The most straightforward estimator is the sample covariance matrix, which is defined as follows:
\begin{equation}
{\hat{\Sigma}_{X}}=\frac{1}{T-1}({X}-\bar{{X}})({X}-\bar{{X}})^{\intercal}=[\sigma_{ij}]_{N\times N},
\end{equation}
where ${X}$ is the $N\times T$ matrix of observations; $\bar{{X}}=\frac{1}{T}{X}{1}_{T}{1}^{\intercal}_{T}$, with ${1}_{T}$ being a $T\times 1$ vectors of 1. However, in the high-dimensional case, where the dimension $N$ goes to infinity at a faster rate than that of the sample size $T$, the naive sample covariance matrix is ill-conditioned. Some structures need to be imposed on ${\Sigma}_{X}$, and regularisation techniques need to be applied to make sure the estimator is reliable. In particular, we usually assume that ${\Sigma}_{X}$ is sparse (i.e., has lots of zeros) or conditionally sparse (i.e., has lots of zeros once we condition on some variables like the common risk factors).

To proceed under sparsity, the key job is to find out the location of non-zero entries. There are two broad categories of regularisation, namely banding and shrinkage.  Banding is applicable when $X_{it}$ is indexed, and we can expect that larger $|i-j|$ implies lower correlation. Such structure is appropriate for applications where there are natural orderings of variables, such as climatology and spectroscopy. \cite{bickel2008regularized} regularize the ${\hat{\Sigma}_{X}}$ in a way that, for a typical entity $i$, only the set of neighours defined by $\{j\in {N_i}: |i-j|\leq \rho, j=1,\dots,N \}$ have non-zero covariances with $i$. Other $j$s with $|i-j|>k$ have $\sigma_{ij}=0$. When $X_{it}$ is not indexed, we may apply some shrinkage techniques to the off-diagonal elements of ${\hat{\Sigma}_{X}}$, and achieve sparsity by setting relatively unimportant entries that are smaller than a data-driven threshold to zeros.  In general, we have ${\tilde{\Sigma}_{X}}=[\tilde{\sigma}_{ij}]_{N\times N}$, where
\begin{equation}
    \tilde{\sigma}_{ij}=\left\{ \begin{array}{cc}
      \hat{\sigma}_{ii}   & i=j \\
       s_{\lambda}(\hat{\sigma}_{ij})  & i\neq j
    \end{array}\right.,
\end{equation}
where $s_{\lambda}(\cdot)$ is a shrinkage function with $\lambda$ being a tuning parameter, satisfying (1) $|s_{\lambda}(\mu)|\leq|\mu|$ for $\mu \in \mathcal{R}$; (2) $s_{\lambda}(\mu)$ if $|u|\leq \lambda$; (3) $|s_{\lambda}(\mu)-\mu|\leq \lambda$. \citet{bickel2008covariance} develop a universal threshold for all entries. \citet{cai2011adaptive} establish entry-adaptive threshold to ${\hat{\Sigma}_{X}}$. \citet{fan2013large} argue that common factors should be extract first before applying thresholding selection when there are "extremely spiked" eigenvalues in ${\hat{\Sigma}_{X}}$ (i.e., the covariance matrix is conditionally sparse). 

Another way of shrinkage is via penalization. \citet{yuan2007model}, \citet{d2008first}, \citet{rothman2008sparse} and \citet{friedman2008sparse} among others employ LASSO type penalty to shrink sample covariance matrix. Another seminal penalty choice is SCAD, which is proposed by \citet{fan2001variable}, can be found in \citet{fan2009network} and \citet{lam2009sparsistency}.

However, most of these shrinkage approaches rely on ${X}$ only and ignore information about pairwise relationships beyond the observed data ${X}$, which may suffer a loss especially in a information-abundant era. When $X_{it}$ is not indexed, we are not be able to have strong implications in terms of the locations of non-zeros as in \cite{bickel2008regularized}. However, we do have some idea that who might be connected with whom using augment information apart from ${X}$. To proxy for pairwise connectivity among entities, apart from purely statistical methods, there have been other ways. \citet{hoberg2016text} use textual analysis to identify peers. \cite{kaustia2013common} identify peers analyst co-coverage, and \cite{ge2021news} identify peers using business news co-mentioning. Those network information gathered from other sources can help us to identify the locations of non-zeros apart from the information from ${X}$ itself. Similar location-based thresholding ideas have applied in \cite{fan2016incorporating} and \citet{brownlees2020community}. \cite{fan2016incorporating} apply a hard thresholding method in a way that $\sigma_{ij}=0$ when $i$ and $j$ are from different sector/industry. \citet{brownlees2020community} first detect community structure using a spectral clustering-based procedure, and then apply a block-by-block thresholding to the off-diagonal elements of ${\hat{\Sigma}_{X}}$. In particular, they do not apply and thresholding to $\hat{\sigma}_{ij}$ if $i$ and $j$ are from the same community.

In this paper, we utilize granular network information gathered from other sources that could imply the locations of non-zeros in the de-factored residuals covariance matrix. The first candidate is the new-implied network. It has been documented that common news coverage reveals information about linkages among companies, which are related to many economically important relationships like business alliances, partnerships, banking and financing, customer-supplier, and production similarity (\citet{scherbina2015economic}, \citet{schwenkler2019network}). \cite{ge2021news} document that stocks linked by news co-mentioning exhibit additional co-movement beyond what can be explained by common risk factors. Same as \cite{ge2021news}, we use news data from RavenPack Equity files Dow Jones Edition for the period between the beginning of 2004 to the end of 2015. This comprehensive news dataset combines relevant content from multiple sources, including Dow Jones Newswires, Wall Street Journal, and Barron’s  MarketWatch, which produce the most actively monitored streams of news articles in the financial system. We identify linkages among firms by news co-mentioning. The second candidate network is IBES analyst co-coverage network. \cite{israelsen2016does} documents that stocks linked by analysts exhibit excess comovement. To construct the analyst co-coverage-based adjacency matrix, we use the Institutional Brokers Estimate System (IBES) detail history files. (after getting the network, our procedure)?


Although here we are applying augment network information to the estimation of large static covariance matrix, similar idea can be extended to the estimation of large dynamic covariance matrix. For example, dynamic network information could be well incorporated into the conditioning information set in \citet{chen2019new}.


\section{Data}
We consider daily returns of $S\& P$ $500$ stocks for our application. All the stock market related data are from the Center for Research in Security Prices (CRSP). Daily factor returns are obtained from Kenneth French’s website.
\subsection{News Implied Network}
The news data are obtained from RavenPack Equity files Dow Jones Edition for the period January 2004 to December 2015. This comprehensive news dataset combines relevant content from multiple sources, including Dow Jones Newswires, Wall Street Journal, and Barron's MarketWatch, which produce the most actively monitored streams of news articles in the financial system. Each unique news story (identified by a unique story ID) tags the companies mentioned in the news by their unique and permanent entity identifier codes (RP\_ENTITY\_ID),  by which we link to stock identifier TICKER and PERMNO.

As as \cite{ge2021news}, we identify links by news co-mentioning. That is, if a piece of business news reports two companies together, they share a link. We do not consider news that co-mention more than two companies since although news they may carry potential information about links, they provide noisier information. We also remove news with topics including analyst recommendations, rating changes, and index movements as these types of news might stack multiple companies together when they actually do not have real links. \autoref{table:news} provides descriptive statistics for RavenPack Equity files Dow Jones Edition dataset during the sample period. Since our comprehensive news dataset combines several sources, given a similar length of sample period, the number of unique news stories is more than ten times larger than that from \citet{scherbina2015economic} and more than eight hundred times than that from \citet{schwenkler2019network}. For link identification purposes, we only use sample news (1) are not about topics mentioned above (2) tag $S\& P$ $500$ companies and (3) mention exactly two companies, which is a subsample of $1,637,256$ unique news stories.

\subsection{IBES Analyst Coverage Network}
We use the Institutional Brokers Estimate System (IBES) detail history files to construct the analyst co-coverage-based adjacency matrix. For each year in the sample, we consider a stock is covered by an analyst if the analyst issues at least one FY1 or FY2 earnings forecast for the stock during the year. And we consider two stocks as linked if there are common analysts during the year, weighted by the number of common analysts. 
