Covariance matrix estimation is an important area of research in both finance and statistics. Suppose we have independent observations ${X}_{t}=(X_{1t},\dots,X_{Nt})^{\intercal}$, $t=1,\dots,T$ of a $N-$dimensional random vector \(\mathbf{X}\) that has mean \(\mu\) and variance ${\Sigma}_{X} =E\pqty{(\mathbf{X} - \mu) \pqty{\mathbf{X} - \mu}^{\intercal}}$. The most straightforward estimator is the sample covariance estimator, which is defined as follows:
\begin{equation}
{\hat{\Sigma}_{X}}=\frac{1}{T}({X}-\bar{{X}})({X}-\bar{{X}})^{\intercal}=[\hat{\sigma}_{ij}]_{N\times N},
\end{equation}
where ${X}$ is the $N\times T$ matrix of observations, $\bar{{X}}=\frac{1}{T}{X}{1}_{T}{1}_{T}^{\intercal}$ is the sample time series average, with ${1}_{T}$ being a $T\times 1$ vector of 1. However, in the high-dimensional case, where the dimension $N$ is not negligible comparing to sample size $T$, the sample covariance matrix is ill-conditioned and inconsistent. Some structures need to be imposed on  ${\Sigma}_{X}$, and regularization techniques need to be applied to make sure the estimator is reliable. 

One of the structures that is often imposed in the high-dimensional settings is sparsity, which assumes that ${\Sigma}_{X}$ is sparse (i.e., has lots of zeros or small elements) or conditionally sparse (i.e., has lots of zeros or small elements once we condition on some variables like the common risk factors). Given the sparsity structure, several estimation strategies have been proposed in the literature such as banding, tapering, shrinkage, thresholding, etc. 

Banding and tapering are applicable when we can find a ``distance'' \(d(i,j)\) between \(X_{i}\) and \(X_{j}\). For example, in time series, a natural distance is $|i-j|$ and we have reasons to believe that a large distance between \(X_{i}\) and \(X_{j}\) will imply a lower correlation. Such structure is appropriate for applications where there are natural orderings of variables, such as time series, climatology and spectroscopy. Banding methods (\cite{bickel2008RegularizedEstimation}) keep only elements within a \(k\)-neighbourhood of each individual \(X_{i}\), that is 
\begin{equation*}
    \hat{\Sigma}_{B,k} = [\hat{\sigma}_{ij} \mathbf{1}_{d(i,j) \leq k}],
\end{equation*}
whereas the tapering estimator does an element-wise multiplication of \(\hat{\Sigma}_{X}\) with a tapering positive definite matrix \(\mathbf{T}\) that has smaller element \(\mathbf{T}_{ij}\) decreases with \(d(i,j)\). 

When we do not have the information about the distance between $i$ and $j$, one of the viable approaches is thresholding. Let \(s_{\lambda}(\cdot)\) be a generalized thresholding operator with thresholding level \(\lambda\), such that for all \(z \in \mathbb{R}\),
\begin{enumerate*}[(1).]
    \item \(\abs{s_{\lambda}(z)} \leq \abs{z}\) for all \(z\)
    \item \(s_{\lambda}(z) = 0\) for \(\abs{z}\leq \lambda\)
    \item \(\abs{s_{\lambda}(z) - z} \leq \lambda\),
\end{enumerate*}
which incorporates commonly used thresholding operators such as hard thresholding, soft thresholding, SCAD, etc. Then a thresholding estimator is 
\begin{equation*}
    \hat{\Sigma}_{\lambda} = \bqty{\tilde{\sigma}_{ij}} \qq{and} \tilde{\sigma}_{ij}= 
    \begin{cases}
        \hat{\sigma}_{ii}   & i=j \\ 
        s_{\lambda}(\hat{\sigma}_{ij})  & i\neq j 
    \end{cases}
\end{equation*}

\cite{bickel2008CovarianceRegularization} develop the theory for universal thresholding, which assumes that the diagonal of \(\Sigma\) is uniformly bounded. \cite{cai2011AdaptiveThresholding} propose an adaptive thresholding estimator. They relax the uniform boundedness assumption and account for the variance of the estimator of each \(\hat{\sigma}_{ij}\) to establish entry-adaptive threshold level ${\hat{\Sigma}_{X}}$. \cite{fan2013large} argue that common factors should be extracted first before applying thresholding when there are "extremely spiked" eigenvalues in ${\hat{\Sigma}_{X}}$ and the covariance matrix is conditionally sparse. \cite{shu2019EstimationLarge} obtain the convergence rate allowing for temporal dependence. \cite{bickel2008CovarianceRegularization} also compare the convergence rates of banding estimator and thresholding estimator. By utilizing the location information, banding estimator shows a superior convergence rate. See \cite{fan2015OverviewEstimation} for a review on estimation of large dimensional covariance matrices. 

Apart from element-wise regularization methods, \cite{ledoit2004WellconditionedEstimator} and \cite{ledoit2012NonlinearShrinkage} have proposed linear and nonlinear shrinkage estimators that apply shrinkage to the eigenvalues of the sample covariance matrix. The linear shrinkage does that by finding the linear combination of sample covariance and a well-conditioned matrix such as the identity matrix and nonlinear shrinkage estimator corrects the eigenvalues using the asymptotic Marcenko–Pastur distribution. Shrinkage estimators have been successfully applied in portfolio construction (\cite{ledoit2004HoneyShrunk}, \cite{ledoit2017NonlinearShrinkage}).

% \cite{bickel2008RegularizedEstimation} regularize the ${\hat{\Sigma}_{X}}$ in a way that, for a typical entity $i$, only the set of neighours defined by $\{j\in {N_i}: |i-j|\leq \rho, j=1,\dots,N \}$ have non-zero covariances with $i$. Other $j$s with $|i-j|>k$ have $\sigma_{ij}=0$. When $X_{it}$ is not indexed, we may apply some shrinkage techniques to the off-diagonal elements of ${\hat{\Sigma}_{X}}$, and achieve sparsity by setting relatively unimportant entries that are smaller than a data-driven threshold to zeros.  In general, we have ${\tilde{\Sigma}_{X}}=[\tilde{\sigma}_{ij}]_{N\times N}$, where
% where $s_{\lambda}(\cdot)$ is a shrinkage function with $\lambda$ being a tuning parameter, satisfying (1) $|s_{\lambda}(\mu)|\leq|\mu|$ for $\mu \in \mathcal{R}$; (2) $s_{\lambda}(\mu)$ if $|u|\leq \lambda$; (3) $|s_{\lambda}(\mu)-\mu|\leq \lambda$.  

% Another way of shrinkage is via penalization. \cite{yuan2007model}, \cite{d2008first}, \cite{rothman2008sparse} and \cite{friedman2008sparse} among others employ LASSO type penalty to shrink sample covariance matrix. Another seminal penalty choice is SCAD, which is proposed by \cite{fan2001variable}, can be found in \cite{fan2009network} and \cite{lam2009sparsistency}.

Although in general we won't have an ordering or the distance between $i$ and $j$, we do have some idea about who might be connected with whom using auxiliary information apart from the observations of ${X}$. To proxy for pairwise connectivity among entities, several methods have been proposed. 
\cite{hoberg2016text} use textual analysis to identify peers. \cite{kaustia2013common} identify peers by analyst co-coverage, and \cite{ge2021news} identify peers using business news co-mentioning. 
The network information gathered from these sources can help us to identify the locations of non-zero elements. Similar location-based thresholding ideas have applied in \cite{fan2016incorporating} and \cite{brownlees2020community}. \cite{fan2016incorporating} apply a hard thresholding method in a way that $\sigma_{ij}=0$ when $i$ and $j$ are from different sector/industry. The network they use is a time-invariant block-diagonal matrix, and our method can accommodate more general and flexible network information.
\cite{brownlees2020community} first detect community structure using a spectral clustering-based procedure, and then apply a block-by-block thresholding to the off-diagonal elements of ${\hat{\Sigma}_{X}}$. In particular, they do not apply thresholding to $\hat{\sigma}_{ij}$ if $i$ and $j$ are from the same community.



% These estimators use only the observations ${X}$ and ignore information about pairwise relationships beyond the observed data ${X}$, which may suffer a loss especially in a information-abundant era. When $X_{it}$ is not indexed, we are not be able to have strong implications in terms of the locations of non-zeros as in \cite{bickel2008regularized}.

In this paper, we use granular network information gathered from other sources. We argue that we can incorporate such auxiliary information in the estimation of the covariance matrix \(\Sigma\) when they help reveal the locations of the large elements (or nonzero elements in the strictly sparse case). We propose a \textit{Network Guided Estimator} that combines the banding and thresholding procedures. Given a network \(\hat{L}\), we keep the elements \(\hat{\sigma}_{ij}\) where \(\hat{L}_{ij} =1\) and apply generalized thresholding operator on the elements \(\hat{\sigma}_{ij}\) where \(\hat{L}_{ij} = 0\). This estimator will relax the sparsity conditions on the covariance \(\Sigma\). In the simulations, we show that the Network Guided Estimator has smaller estimation error comparing to the sample covariance, linear shrinkage, nonlinear shrinakge and the universal thresholding estimators. 

As an empirical application, we use the Network Guided Estimator to estimate the covariance of excess returns of SP500 stocks, and construct the global minimum variance portfolio. The first source of location information comes from the new-implied network. It has been documented that common news coverages reveal information about linkages among companies, which are related to many economically important relationships like business alliances, partnerships, banking and financing, customer-supplier, and production similarity (\cite{scherbina2015economic}, \cite{schwenkler2019network}). \cite{ge2021news} document that stocks linked by news co-mentioning exhibit additional co-movement beyond what can be explained by common risk factors. Same as \cite{ge2021news}, we use news data from RavenPack Equity files Dow Jones Edition for the period from the beginning of 2004 to the end of 2015. This comprehensive news dataset combines relevant contents from multiple sources, including Dow Jones Newswires, Wall Street Journal, and Barron’s  MarketWatch, which produce the most actively monitored streams of news articles in the financial system. We identify linkages among firms by news co-mentioning. 

As a simple starting point, in each rolling window, we set \(\hat{L}_{ij} = 1\) if the firms \((i,j)\) are co-mentioned more than 50 times during the past year and their current sample correlation reaches a certain level. We apply the Network Guided Estimator to the de-factored excess returns and show that the global minimum variance portfolio that we constructed have smaller standard deviation than portfolios constructed using other methods. This empirical analysis is still preliminary and we are looking into ways to construct better estimator \(\hat{L}_{ij}\). 

% The second candidate network is Institutional Brokers Estimate System (IBES) analyst co-coverage network. \cite{israelsen2016does} documents that stocks linked by analysts exhibit excess comovement. To construct the analyst co-coverage-based adjacency matrix, we use the Institutional Brokers Estimate System (IBES) detail history files. (after getting the network, our procedure? key results?)



