Assume we have observations \(X = \pqty{X_{1}, \dots, X_{T}}\), where \(X_{t}\) are independent drawn from a \(N\)-dimensional distribution \(F\) with mean \(\mu\) and variance \(\Sigma\). In \cite{bickel2008CovarianceRegularization}, they consider the following uniformity class of covariance matrices:
\begin{equation*}
    \mathcal{U}_{\tau}(q, c_{0}(p) , M) = \Bqty{\Sigma: \sigma_{i,i} \leq M, \sum_{j=1}^{p} \abs{\sigma_{ij}}^{q} \leq c_{0}(p), \mbox{ for all } i}
\end{equation*}
And the convergence rate will depend on \(c_{0}(p)\) and \(q\). Notice that in order to bound the \(i\)-th row sparsity index \(\mathcal{S}_{i}(q) := \sum_{j=1}^{p} \abs{\sigma_{ij}}^{q}\) by \(c_{0}(p)\), the coefficient \(q\) is important. Suppose \(q = 0\), then \(\mathcal{S}_{i} = \#\Bqty{\sigma_{ij} \neq 0}\). If the \(i\)-th row \(\Sigma_{i\cdot}\) has a lot of nonzero but small elements, then to bound \(\mathcal{S}_{i}(0)\) would require a higher \(c_{0}(p)\). On the other hand when \(q \to 1\), the large elements of \(\Sigma_{i \cdot}\) will dominate. Hence if \(\Sigma\) is sparse in the sense that it contains a small number of relatively large elements and a large number of small elements, it's advisable to state conditions separately for these elements, and we consider the following uniformity class:
\begin{align*}
    \mathcal{U}(q, c_{0}, c_{1}, M, L) = \Bqty{\Sigma : \sigma_{ii} \leq M, \sum_{j} L_{ij}^{1} \leq c_{1}(p), \sum_{j} L_{ij}^{0}\abs{\sigma_{ij}}^{q} \leq c_{0}(p) \mbox{ for all } i}
\end{align*}
where \(L_{ij}\) represents the location of the large elements and for \(s \in \Bqty{0,1}\), \(L_{ij}^{s} = \mathbf{1}_{L_{ij} = s}\) This uniformity class controls the number of the large elements at locations \(((i,j): L_{ij}^{1} = 1)\) and the growth rate of the remaining small elements. 

Of course, a priori we don't know the location of the large elements, but suppose we have observations from auxiliary dataset that allow us to form an estimator \(\hat{L}\) for \(L\), independent of the sample \(X\), we can design an estimator that takes into account the addition information in \(\hat{L}\). A simple choice is to do banding based on the location information and apply thresholding on the reminder terms. Here we define a \textit{Network Guided Estimator} to be
\begin{align*}
    T_{L,\lambda}(\hat{\Sigma}) &= \bqty{s_{L, \lambda } (\hat{\sigma}_{ij})}_{N\times N}\\
    s_{L,\lambda} \pqty{\sigma_{ij}} &= 
    \begin{cases}
        \sigma_{ij} \qq{if} i = j \qq{or} L_{ij} = 1\\ 
        s_{\lambda}(\sigma_{ij}) \qq{otherwise} 
    \end{cases}
\end{align*}
where \(s_{\lambda}(x)\) is the generalized thresholding operator and \(\hat{\sigma}_{ij}\) are elements of the sample covariance matrix, then the feasible Network Guided Estimator is \(T_{\hat{L},\lambda}\pqty{\hat{\Sigma}}\)

\begin{asmp}\label{asmp:1}
    We make the following assumptions:
    \begin{enumerate}
        \item \(\max_{ij} \abs{\hat{\sigma}_{ij} - \sigma_{ij}} = O_{p}(\sqrt{\log N / T})\);
        \item \(\max_{ij} \abs{\hat{L}_{ij} - L_{ij}} = O_{p}(k_{T})\) where \(k_{T} \to 0\) as \(T \to \infty\).
    \end{enumerate}
\end{asmp}
\begin{remark}
    \begin{enumerate}
        \item 
        The first assumption can be verified in various settings, for example, if \(F\) is Gaussian or sub-Gaussianv(\cite{cai2011AdaptiveThresholding}). We can even replace the independence assumption and allow for temporal dependence (see Lemma A.2 in \cite{shu2019EstimationLarge}). 
        \item The second assumption appears restrictive, but since \(L_{ij}\) are estimated independently from different datasets, we believe it's not too strigent to require that for each \((i,j)\), \(L_{ij}\) can be estimated consistently. In addition, simulation shows that as long as we don't make too much II-type error: \(\hat{L}_{ij} = 1\) when \(L_{ij} = 0\), using the additional information still improves the performance. 
        
        Given that the estimation of \(\hat{L}_{ij}\) is independent of the sample \((X_{t})\), then perhaps we can find a less restrictive condition for the result to hold.
    \end{enumerate}
\end{remark}


\begin{thm}
    Suppose \(F\) is Gaussian and for sufficiently large \(M\), 
    \begin{equation*}
        \lambda = M \sqrt{\frac{\log N}{T}}
    \end{equation*}
    and \(\frac{\log N}{T} \to 0\) as \(T \to \infty\), 
    then for the operator norm \(\norm{M}= \max_{j} \abs{\lambda_{j}(M)}\) where \(\lambda_{1},\dots,\lambda_{N}\) are the eigenvalues of \(M\), we have 
    \begin{equation*}
        \norm{T_{\hat{L},\lambda}\pqty{\hat{\Sigma}} - \Sigma } = O_{p}\pqty{c_{1}(p) \sqrt{\frac{\log N}{T}}+c_{0}(p)\pqty{\frac{\log N}{T}}^{\frac{1-q}{2}}}. 
    \end{equation*}
\end{thm}
